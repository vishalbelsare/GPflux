
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>gpflux.layers &#8212; GPflux 0.1.0 documentation</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/graphviz.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/pydata-custom.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="gpflux.layers.basis_functions" href="basis_functions/index.html" />
    <link rel="prev" title="gpflux.experiment_support.tensorboard" href="../experiment_support/tensorboard/index.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="../../../index.html">
  <img src="../../../_static/logo.png" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../index.html">
  GPflux
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../notebooks/benchmarks.html">
  Benchmarks
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../tutorials.html">
  Tutorials
 </a>
</li>

<li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="../index.html">
  API Reference
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/secondmind-labs/gpflux" rel="noopener" target="_blank" title="GitHub"><span><i class="fab fa-github-square"></i></span>
            <label class="sr-only">GitHub</label></a>
        </li>
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar">
              <div class="sidebar-start-items"><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../architectures/index.html">
   <code class="xref py py-mod docutils literal notranslate">
    <span class="pre">
     gpflux.architectures
    </span>
   </code>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../architectures/constant_input_dim_deep_gp/index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       gpflux.architectures.constant_input_dim_deep_gp
      </span>
     </code>
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../encoders/index.html">
   <code class="xref py py-mod docutils literal notranslate">
    <span class="pre">
     gpflux.encoders
    </span>
   </code>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../encoders/directly_parameterized_encoder/index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       gpflux.encoders.directly_parameterized_encoder
      </span>
     </code>
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../experiment_support/index.html">
   <code class="xref py py-mod docutils literal notranslate">
    <span class="pre">
     gpflux.experiment_support
    </span>
   </code>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../experiment_support/ci_utils/index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       gpflux.experiment_support.ci_utils
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../experiment_support/tensorboard/index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       gpflux.experiment_support.tensorboard
      </span>
     </code>
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="current reference internal" href="#">
   <code class="xref py py-mod docutils literal notranslate">
    <span class="pre">
     gpflux.layers
    </span>
   </code>
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="basis_functions/index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       gpflux.layers.basis_functions
      </span>
     </code>
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
    <label for="toctree-checkbox-5">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="basis_functions/fourier_features/index.html">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         gpflux.layers.basis_functions.fourier_features
        </span>
       </code>
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
      <label for="toctree-checkbox-6">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="basis_functions/fourier_features/quadrature/index.html">
         <code class="xref py py-mod docutils literal notranslate">
          <span class="pre">
           gpflux.layers.basis_functions.fourier_features.quadrature
          </span>
         </code>
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="basis_functions/fourier_features/random/index.html">
         <code class="xref py py-mod docutils literal notranslate">
          <span class="pre">
           gpflux.layers.basis_functions.fourier_features.random
          </span>
         </code>
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="basis_functions/fourier_features/base/index.html">
         <code class="xref py py-mod docutils literal notranslate">
          <span class="pre">
           gpflux.layers.basis_functions.fourier_features.base
          </span>
         </code>
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="basis_functions/fourier_features/utils/index.html">
         <code class="xref py py-mod docutils literal notranslate">
          <span class="pre">
           gpflux.layers.basis_functions.fourier_features.utils
          </span>
         </code>
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="bayesian_dense_layer/index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       gpflux.layers.bayesian_dense_layer
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="gp_layer/index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       gpflux.layers.gp_layer
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="latent_variable_layer/index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       gpflux.layers.latent_variable_layer
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="likelihood_layer/index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       gpflux.layers.likelihood_layer
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="trackable_layer/index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       gpflux.layers.trackable_layer
      </span>
     </code>
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../models/index.html">
   <code class="xref py py-mod docutils literal notranslate">
    <span class="pre">
     gpflux.models
    </span>
   </code>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../models/deep_gp/index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       gpflux.models.deep_gp
      </span>
     </code>
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../optimization/index.html">
   <code class="xref py py-mod docutils literal notranslate">
    <span class="pre">
     gpflux.optimization
    </span>
   </code>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../optimization/keras_natgrad/index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       gpflux.optimization.keras_natgrad
      </span>
     </code>
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../sampling/index.html">
   <code class="xref py py-mod docutils literal notranslate">
    <span class="pre">
     gpflux.sampling
    </span>
   </code>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../sampling/kernel_with_feature_decomposition/index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       gpflux.sampling.kernel_with_feature_decomposition
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../sampling/sample/index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       gpflux.sampling.sample
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../sampling/utils/index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       gpflux.sampling.utils
      </span>
     </code>
    </a>
   </li>
  </ul>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../callbacks/index.html">
   <code class="xref py py-mod docutils literal notranslate">
    <span class="pre">
     gpflux.callbacks
    </span>
   </code>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exceptions/index.html">
   <code class="xref py py-mod docutils literal notranslate">
    <span class="pre">
     gpflux.exceptions
    </span>
   </code>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../helpers/index.html">
   <code class="xref py py-mod docutils literal notranslate">
    <span class="pre">
     gpflux.helpers
    </span>
   </code>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../losses/index.html">
   <code class="xref py py-mod docutils literal notranslate">
    <span class="pre">
     gpflux.losses
    </span>
   </code>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../math/index.html">
   <code class="xref py py-mod docutils literal notranslate">
    <span class="pre">
     gpflux.math
    </span>
   </code>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../runtime_checks/index.html">
   <code class="xref py py-mod docutils literal notranslate">
    <span class="pre">
     gpflux.runtime_checks
    </span>
   </code>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../types/index.html">
   <code class="xref py py-mod docutils literal notranslate">
    <span class="pre">
     gpflux.types
    </span>
   </code>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../version/index.html">
   <code class="xref py py-mod docutils literal notranslate">
    <span class="pre">
     gpflux.version
    </span>
   </code>
  </a>
 </li>
</ul>

  </div>
</nav>
              </div>
              <div class="sidebar-end-items">
              </div>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage mt-5 pt-1 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#subpackages">
   Subpackages
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#submodules">
   Submodules
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#package-contents">
   Package Contents
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gpflux.layers.BayesianDenseLayer">
     BayesianDenseLayer
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gpflux.layers.BayesianDenseLayer.build">
       build
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gpflux.layers.BayesianDenseLayer.predict_samples">
       predict_samples
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gpflux.layers.BayesianDenseLayer.call">
       call
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gpflux.layers.BayesianDenseLayer.prior_kl">
       prior_kl
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gpflux.layers.GPLayer">
     GPLayer
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gpflux.layers.GPLayer.num_data">
       num_data
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gpflux.layers.GPLayer.whiten">
       whiten
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gpflux.layers.GPLayer.num_samples">
       num_samples
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gpflux.layers.GPLayer.full_cov">
       full_cov
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gpflux.layers.GPLayer.full_output_cov">
       full_output_cov
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gpflux.layers.GPLayer.q_mu">
       q_mu
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gpflux.layers.GPLayer.q_sqrt">
       q_sqrt
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gpflux.layers.GPLayer.predict">
       predict
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gpflux.layers.GPLayer.call">
       call
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gpflux.layers.GPLayer.prior_kl">
       prior_kl
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gpflux.layers.GPLayer._make_distribution_fn">
       _make_distribution_fn
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gpflux.layers.GPLayer._convert_to_tensor_fn">
       _convert_to_tensor_fn
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gpflux.layers.GPLayer.sample">
       sample
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gpflux.layers.LatentVariableLayer">
     LatentVariableLayer
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gpflux.layers.LatentVariableLayer.prior">
       prior
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gpflux.layers.LatentVariableLayer.encoder">
       encoder
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gpflux.layers.LatentVariableLayer.compositor">
       compositor
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gpflux.layers.LatentVariableLayer.call">
       call
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gpflux.layers.LatentVariableLayer._inference_posteriors">
       _inference_posteriors
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gpflux.layers.LatentVariableLayer._inference_latent_samples_and_loss">
       _inference_latent_samples_and_loss
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gpflux.layers.LatentVariableLayer._prediction_latent_samples">
       _prediction_latent_samples
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gpflux.layers.LatentVariableLayer._local_kls">
       _local_kls
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gpflux.layers.LayerWithObservations">
     LayerWithObservations
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gpflux.layers.LayerWithObservations.call">
       call
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gpflux.layers.LikelihoodLayer">
     LikelihoodLayer
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gpflux.layers.LikelihoodLayer.call">
       call
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gpflux.layers.TrackableLayer">
     TrackableLayer
    </a>
   </li>
  </ul>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <section id="module-gpflux.layers">
<span id="gpflux-layers"></span><h1><a class="reference internal" href="#module-gpflux.layers" title="gpflux.layers"><code class="xref py py-mod docutils literal notranslate"><span class="pre">gpflux.layers</span></code></a><a class="headerlink" href="#module-gpflux.layers" title="Permalink to this headline">#</a></h1>
<p>Layers</p>
<section id="subpackages">
<h2>Subpackages<a class="headerlink" href="#subpackages" title="Permalink to this headline">#</a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="basis_functions/index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">gpflux.layers.basis_functions</span></code></a><ul>
<li class="toctree-l2"><a class="reference internal" href="basis_functions/fourier_features/index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">gpflux.layers.basis_functions.fourier_features</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="basis_functions/fourier_features/quadrature/index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">gpflux.layers.basis_functions.fourier_features.quadrature</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="basis_functions/fourier_features/random/index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">gpflux.layers.basis_functions.fourier_features.random</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="basis_functions/fourier_features/base/index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">gpflux.layers.basis_functions.fourier_features.base</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="basis_functions/fourier_features/utils/index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">gpflux.layers.basis_functions.fourier_features.utils</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</section>
<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">#</a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="bayesian_dense_layer/index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">gpflux.layers.bayesian_dense_layer</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="gp_layer/index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">gpflux.layers.gp_layer</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="latent_variable_layer/index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">gpflux.layers.latent_variable_layer</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="likelihood_layer/index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">gpflux.layers.likelihood_layer</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="trackable_layer/index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">gpflux.layers.trackable_layer</span></code></a></li>
</ul>
</div>
</section>
<section id="package-contents">
<h2>Package Contents<a class="headerlink" href="#package-contents" title="Permalink to this headline">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="gpflux.layers.BayesianDenseLayer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">BayesianDenseLayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_data</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">w_mu</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.22)"><span class="pre">numpy.ndarray</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">w_sqrt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.22)"><span class="pre">numpy.ndarray</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_mean_field</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temperature</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><span class="pre">float</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0001</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/gpflux/layers/bayesian_dense_layer.html#BayesianDenseLayer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#gpflux.layers.BayesianDenseLayer" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="trackable_layer/index.html#gpflux.layers.trackable_layer.TrackableLayer" title="gpflux.layers.trackable_layer.TrackableLayer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">gpflux.layers.trackable_layer.TrackableLayer</span></code></a></p>
<p>A dense (fully-connected) layer for variational Bayesian neural networks.</p>
<p>This layer holds the mean and square-root of the variance of the
distribution over the weights. This layer also has a temperature for
cooling (or heating) the posterior.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dim</strong> – The input dimension (excluding bias) of this layer.</p></li>
<li><p><strong>output_dim</strong> – The output dimension of this layer.</p></li>
<li><p><strong>num_data</strong> – The number of points in the training dataset (used for
scaling the KL regulariser).</p></li>
<li><p><strong>w_mu</strong> – Initial value of the variational mean for weights + bias.
If not specified, this defaults to <a class="reference internal" href="../helpers/index.html#gpflux.helpers.xavier_initialization_numpy" title="gpflux.helpers.xavier_initialization_numpy"><code class="xref any py py-func docutils literal notranslate"><span class="pre">xavier_initialization_numpy</span></code></a>
for the weights and zero for the bias.</p></li>
<li><p><strong>w_sqrt</strong> – Initial value of the variational Cholesky of the
(co)variance for weights + bias. If not specified, this defaults to
1e-5 * Identity.</p></li>
<li><p><strong>activation</strong> – The activation function. If not specified, this defaults to the identity.</p></li>
<li><p><strong>is_mean_field</strong> – Determines whether the approximation to the
weight posterior is mean field. Must be consistent with the shape
of <code class="docutils literal notranslate"><span class="pre">w_sqrt</span></code>, if specified.</p></li>
<li><p><strong>temperature</strong> – The KL loss will be scaled by this factor.
Can be used for cooling (&lt; 1.0) or heating (&gt; 1.0) the posterior.
As suggested in <a class="reference external" href="http://proceedings.mlr.press/v119/wenzel20a">“How Good is the Bayes Posterior in Deep Neural
Networks Really?” by Wenzel et al. (2020)</a> the default value
is a cold <code class="docutils literal notranslate"><span class="pre">1e-4</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="gpflux.layers.BayesianDenseLayer.build">
<span class="sig-name descname"><span class="pre">build</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_shape</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">gpflux.types.ShapeType</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.10)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#gpflux.layers.BayesianDenseLayer.build" title="Permalink to this definition">#</a></dt>
<dd><p>Build the variables necessary on first call</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="gpflux.layers.BayesianDenseLayer.predict_samples">
<span class="sig-name descname"><span class="pre">predict_samples</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">gpflow.base.TensorType</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_samples</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><span class="pre">int</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/Tensor" title="(in TensorFlow v2.4)"><span class="pre">tf.Tensor</span></a></span></span><a class="headerlink" href="#gpflux.layers.BayesianDenseLayer.predict_samples" title="Permalink to this definition">#</a></dt>
<dd><p>Samples from the approximate posterior at N test inputs, with input_dim = D, output_dim = Q.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> – The inputs to predict at; shape <code class="docutils literal notranslate"><span class="pre">[N,</span> <span class="pre">D]</span></code>.</p></li>
<li><p><strong>num_samples</strong> – The number of samples S, to draw.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Samples, shape <code class="docutils literal notranslate"><span class="pre">[S,</span> <span class="pre">N,</span> <span class="pre">Q]</span></code> if S is not None else <code class="docutils literal notranslate"><span class="pre">[N,</span> <span class="pre">Q]</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="gpflux.layers.BayesianDenseLayer.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">gpflow.base.TensorType</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><span class="pre">bool</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/Tensor" title="(in TensorFlow v2.4)"><span class="pre">tf.Tensor</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">gpflow.models.model.MeanAndVariance</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#gpflux.layers.BayesianDenseLayer.call" title="Permalink to this definition">#</a></dt>
<dd><p>The default behaviour upon calling this layer.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="gpflux.layers.BayesianDenseLayer.prior_kl">
<span class="sig-name descname"><span class="pre">prior_kl</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/Tensor" title="(in TensorFlow v2.4)"><span class="pre">tf.Tensor</span></a></span></span><a class="headerlink" href="#gpflux.layers.BayesianDenseLayer.prior_kl" title="Permalink to this definition">#</a></dt>
<dd><p>Returns the KL divergence <code class="docutils literal notranslate"><span class="pre">KL[q(u)∥p(u)]</span></code> from the prior <code class="docutils literal notranslate"><span class="pre">p(u)</span> <span class="pre">=</span> <span class="pre">N(0,</span> <span class="pre">I)</span></code> to
the variational distribution <code class="docutils literal notranslate"><span class="pre">q(u)</span> <span class="pre">=</span> <span class="pre">N(w_mu,</span> <span class="pre">w_sqrt²)</span></code>.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="gpflux.layers.GPLayer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">GPLayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">kernel</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">gpflow.kernels.MultioutputKernel</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inducing_variable</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">gpflow.inducing_variables.MultioutputInducingVariables</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_data</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">mean_function</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">gpflow.mean_functions.MeanFunction</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_samples</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><span class="pre">int</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_cov</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_output_cov</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_latent_gps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><span class="pre">int</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">whiten</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/gpflux/layers/gp_layer.html#GPLayer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#gpflux.layers.GPLayer" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference external" href="https://www.tensorflow.org/probability/api_docs/python/tfp/layers/DistributionLambda" title="(in TensorFlow Probability v0.12)"><code class="docutils literal notranslate"><span class="pre">tfp.layers.DistributionLambda</span></code></a></p>
<p>A sparse variational multioutput GP layer. This layer holds the kernel,
inducing variables and variational distribution, and mean function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>kernel</strong> – The multioutput kernel for this layer.</p></li>
<li><p><strong>inducing_variable</strong> – The inducing features for this layer.</p></li>
<li><p><strong>num_data</strong> – The number of points in the training dataset (see <a class="reference internal" href="#gpflux.layers.GPLayer.num_data" title="gpflux.layers.GPLayer.num_data"><code class="xref py py-attr docutils literal notranslate"><span class="pre">num_data</span></code></a>).</p></li>
<li><p><strong>mean_function</strong> – <p>The mean function that will be applied to the
inputs. Default: <code class="xref py py-class docutils literal notranslate"><span class="pre">Identity</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The Identity mean function requires the input and output
dimensionality of this layer to be the same. If you want to
change the dimensionality in a layer, you may want to provide a
<code class="xref py py-class docutils literal notranslate"><span class="pre">Linear</span></code> mean function instead.</p>
</div>
</p></li>
<li><p><strong>num_samples</strong> – The number of samples to draw when converting the
<a class="reference external" href="https://www.tensorflow.org/probability/api_docs/python/tfp/layers/DistributionLambda" title="(in TensorFlow Probability v0.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistributionLambda</span></code></a> into a <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/Tensor" title="(in TensorFlow v2.4)"><code class="xref any docutils literal notranslate"><span class="pre">tf.Tensor</span></code></a>, see
<a class="reference internal" href="#gpflux.layers.GPLayer._convert_to_tensor_fn" title="gpflux.layers.GPLayer._convert_to_tensor_fn"><code class="xref py py-meth docutils literal notranslate"><span class="pre">_convert_to_tensor_fn()</span></code></a>. Will be stored in the
<a class="reference internal" href="#gpflux.layers.GPLayer.num_samples" title="gpflux.layers.GPLayer.num_samples"><code class="xref py py-attr docutils literal notranslate"><span class="pre">num_samples</span></code></a> attribute.  If <a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.10)"><code class="xref any docutils literal notranslate"><span class="pre">None</span></code></a> (the default), draw a
single sample without prefixing the sample shape (see
<a class="reference external" href="https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/Distribution" title="(in TensorFlow Probability v0.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">tfp.distributions.Distribution</span></code></a>’s <a class="reference external" href="https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/Distribution#sample">sample()</a>
method).</p></li>
<li><p><strong>full_cov</strong> – Sets default behaviour of calling this layer
(<a class="reference internal" href="#gpflux.layers.GPLayer.full_cov" title="gpflux.layers.GPLayer.full_cov"><code class="xref py py-attr docutils literal notranslate"><span class="pre">full_cov</span></code></a> attribute):
If <a class="reference external" href="https://docs.python.org/3/library/constants.html#False" title="(in Python v3.10)"><code class="xref any docutils literal notranslate"><span class="pre">False</span></code></a> (the default), only predict marginals (diagonal
of covariance) with respect to inputs.
If <a class="reference external" href="https://docs.python.org/3/library/constants.html#True" title="(in Python v3.10)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, predict full covariance over inputs.</p></li>
<li><p><strong>full_output_cov</strong> – Sets default behaviour of calling this layer
(<a class="reference internal" href="#gpflux.layers.GPLayer.full_output_cov" title="gpflux.layers.GPLayer.full_output_cov"><code class="xref py py-attr docutils literal notranslate"><span class="pre">full_output_cov</span></code></a> attribute):
If <a class="reference external" href="https://docs.python.org/3/library/constants.html#False" title="(in Python v3.10)"><code class="xref any docutils literal notranslate"><span class="pre">False</span></code></a> (the default), only predict marginals (diagonal
of covariance) with respect to outputs.
If <a class="reference external" href="https://docs.python.org/3/library/constants.html#True" title="(in Python v3.10)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, predict full covariance over outputs.</p></li>
<li><p><strong>num_latent_gps</strong> – The number of (latent) GPs in the layer
(which can be different from the number of outputs, e.g. with a
<code class="xref py py-class docutils literal notranslate"><span class="pre">LinearCoregionalization</span></code> kernel).
This is used to determine the size of the
variational parameters <a class="reference internal" href="#gpflux.layers.GPLayer.q_mu" title="gpflux.layers.GPLayer.q_mu"><code class="xref py py-attr docutils literal notranslate"><span class="pre">q_mu</span></code></a> and <a class="reference internal" href="#gpflux.layers.GPLayer.q_sqrt" title="gpflux.layers.GPLayer.q_sqrt"><code class="xref py py-attr docutils literal notranslate"><span class="pre">q_sqrt</span></code></a>.
If possible, it is inferred from the <em>kernel</em> and <em>inducing_variable</em>.</p></li>
<li><p><strong>whiten</strong> – If <a class="reference external" href="https://docs.python.org/3/library/constants.html#True" title="(in Python v3.10)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a> (the default), uses the whitened parameterisation
of the inducing variables; see <a class="reference internal" href="#gpflux.layers.GPLayer.whiten" title="gpflux.layers.GPLayer.whiten"><code class="xref py py-attr docutils literal notranslate"><span class="pre">whiten</span></code></a>.</p></li>
<li><p><strong>name</strong> – The name of this layer.</p></li>
<li><p><strong>verbose</strong> – The verbosity mode. Set this parameter to <a class="reference external" href="https://docs.python.org/3/library/constants.html#True" title="(in Python v3.10)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>
to show debug information.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="gpflux.layers.GPLayer.num_data">
<span class="sig-name descname"><span class="pre">num_data</span></span><em class="property"><span class="w"> </span><span class="pre">:int</span></em><a class="headerlink" href="#gpflux.layers.GPLayer.num_data" title="Permalink to this definition">#</a></dt>
<dd><p>The number of points in the training dataset. This information is used to
obtain the correct scaling between the data-fit and the KL term in the
evidence lower bound (ELBO).</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="gpflux.layers.GPLayer.whiten">
<span class="sig-name descname"><span class="pre">whiten</span></span><em class="property"><span class="w"> </span><span class="pre">:bool</span></em><a class="headerlink" href="#gpflux.layers.GPLayer.whiten" title="Permalink to this definition">#</a></dt>
<dd><p>This parameter determines the parameterisation of the inducing variables.</p>
<p>If <a class="reference external" href="https://docs.python.org/3/library/constants.html#True" title="(in Python v3.10)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, this layer uses the whitened (or non-centred) representation, in
which (at the example of inducing point inducing variables) <code class="docutils literal notranslate"><span class="pre">u</span> <span class="pre">=</span> <span class="pre">f(Z)</span> <span class="pre">=</span>
<span class="pre">cholesky(Kuu)</span> <span class="pre">v</span></code>, and we parameterise an approximate posterior on <code class="docutils literal notranslate"><span class="pre">v</span></code> as
<code class="docutils literal notranslate"><span class="pre">q(v)</span> <span class="pre">=</span> <span class="pre">N(q_mu,</span> <span class="pre">q_sqrt</span> <span class="pre">q_sqrtᵀ)</span></code>. The prior on <code class="docutils literal notranslate"><span class="pre">v</span></code> is <code class="docutils literal notranslate"><span class="pre">p(v)</span> <span class="pre">=</span> <span class="pre">N(0,</span> <span class="pre">I)</span></code>.</p>
<p>If <a class="reference external" href="https://docs.python.org/3/library/constants.html#False" title="(in Python v3.10)"><code class="xref any docutils literal notranslate"><span class="pre">False</span></code></a>, this layer uses the non-whitened (or centred) representation,
in which we directly parameterise <code class="docutils literal notranslate"><span class="pre">q(u)</span> <span class="pre">=</span> <span class="pre">N(q_mu,</span> <span class="pre">q_sqrt</span> <span class="pre">q_sqrtᵀ)</span></code>. The
prior on <code class="docutils literal notranslate"><span class="pre">u</span></code> is <code class="docutils literal notranslate"><span class="pre">p(u)</span> <span class="pre">=</span> <span class="pre">N(0,</span> <span class="pre">Kuu)</span></code>.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="gpflux.layers.GPLayer.num_samples">
<span class="sig-name descname"><span class="pre">num_samples</span></span><em class="property"><span class="w"> </span><span class="pre">:Optional[int]</span></em><a class="headerlink" href="#gpflux.layers.GPLayer.num_samples" title="Permalink to this definition">#</a></dt>
<dd><p>The number of samples drawn when coercing the output distribution of
this layer to a <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/Tensor" title="(in TensorFlow v2.4)"><code class="xref any docutils literal notranslate"><span class="pre">tf.Tensor</span></code></a>. (See <a class="reference internal" href="#gpflux.layers.GPLayer._convert_to_tensor_fn" title="gpflux.layers.GPLayer._convert_to_tensor_fn"><code class="xref py py-meth docutils literal notranslate"><span class="pre">_convert_to_tensor_fn()</span></code></a>.)</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="gpflux.layers.GPLayer.full_cov">
<span class="sig-name descname"><span class="pre">full_cov</span></span><em class="property"><span class="w"> </span><span class="pre">:bool</span></em><a class="headerlink" href="#gpflux.layers.GPLayer.full_cov" title="Permalink to this definition">#</a></dt>
<dd><p>This parameter determines the behaviour of calling this layer. If <a class="reference external" href="https://docs.python.org/3/library/constants.html#False" title="(in Python v3.10)"><code class="xref any docutils literal notranslate"><span class="pre">False</span></code></a>, only
predict or sample marginals (diagonal of covariance) with respect to inputs.
If <a class="reference external" href="https://docs.python.org/3/library/constants.html#True" title="(in Python v3.10)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, predict or sample with the full covariance over the inputs.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="gpflux.layers.GPLayer.full_output_cov">
<span class="sig-name descname"><span class="pre">full_output_cov</span></span><em class="property"><span class="w"> </span><span class="pre">:bool</span></em><a class="headerlink" href="#gpflux.layers.GPLayer.full_output_cov" title="Permalink to this definition">#</a></dt>
<dd><p>This parameter determines the behaviour of calling this layer. If <a class="reference external" href="https://docs.python.org/3/library/constants.html#False" title="(in Python v3.10)"><code class="xref any docutils literal notranslate"><span class="pre">False</span></code></a>, only
predict or sample marginals (diagonal of covariance) with respect to outputs.
If <a class="reference external" href="https://docs.python.org/3/library/constants.html#True" title="(in Python v3.10)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, predict or sample with the full covariance over the outputs.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="gpflux.layers.GPLayer.q_mu">
<span class="sig-name descname"><span class="pre">q_mu</span></span><em class="property"><span class="w"> </span><span class="pre">:gpflow.Parameter</span></em><a class="headerlink" href="#gpflux.layers.GPLayer.q_mu" title="Permalink to this definition">#</a></dt>
<dd><p>The mean of <code class="docutils literal notranslate"><span class="pre">q(v)</span></code> or <code class="docutils literal notranslate"><span class="pre">q(u)</span></code> (depending on whether <a class="reference internal" href="#gpflux.layers.GPLayer.whiten" title="gpflux.layers.GPLayer.whiten"><code class="xref py py-attr docutils literal notranslate"><span class="pre">whiten</span></code></a>ed
parametrisation is used).</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="gpflux.layers.GPLayer.q_sqrt">
<span class="sig-name descname"><span class="pre">q_sqrt</span></span><em class="property"><span class="w"> </span><span class="pre">:gpflow.Parameter</span></em><a class="headerlink" href="#gpflux.layers.GPLayer.q_sqrt" title="Permalink to this definition">#</a></dt>
<dd><p>The lower-triangular Cholesky factor of the covariance of <code class="docutils literal notranslate"><span class="pre">q(v)</span></code> or <code class="docutils literal notranslate"><span class="pre">q(u)</span></code>
(depending on whether <a class="reference internal" href="#gpflux.layers.GPLayer.whiten" title="gpflux.layers.GPLayer.whiten"><code class="xref py py-attr docutils literal notranslate"><span class="pre">whiten</span></code></a>ed parametrisation is used).</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="gpflux.layers.GPLayer.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">gpflow.base.TensorType</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_cov</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_output_cov</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/Tensor" title="(in TensorFlow v2.4)"><span class="pre">tf.Tensor</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/Tensor" title="(in TensorFlow v2.4)"><span class="pre">tf.Tensor</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#gpflux.layers.GPLayer.predict" title="Permalink to this definition">#</a></dt>
<dd><p>Make a prediction at N test inputs for the Q outputs of this layer,
including the mean function contribution.</p>
<p>The covariance and its shape is determined by <em>full_cov</em> and <em>full_output_cov</em> as follows:</p>
<table class="table">
<colgroup>
<col style="width: 27%" />
<col style="width: 37%" />
<col style="width: 36%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p>(co)variance shape</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">full_output_cov=False</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">full_output_cov=True</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">full_cov=False</span></code></p></td>
<td><p>[N, Q]</p></td>
<td><p>[N, Q, Q]</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">full_cov=True</span></code></p></td>
<td><p>[Q, N, N]</p></td>
<td><p>[N, Q, N, Q]</p></td>
</tr>
</tbody>
</table>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> – The inputs to predict at, with a shape of [N, D], where D is
the input dimensionality of this layer.</p></li>
<li><p><strong>full_cov</strong> – Whether to return full covariance (if <a class="reference external" href="https://docs.python.org/3/library/constants.html#True" title="(in Python v3.10)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>) or
marginal variance (if <a class="reference external" href="https://docs.python.org/3/library/constants.html#False" title="(in Python v3.10)"><code class="xref any docutils literal notranslate"><span class="pre">False</span></code></a>, the default) w.r.t. inputs.</p></li>
<li><p><strong>full_output_cov</strong> – Whether to return full covariance (if <a class="reference external" href="https://docs.python.org/3/library/constants.html#True" title="(in Python v3.10)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>)
or marginal variance (if <a class="reference external" href="https://docs.python.org/3/library/constants.html#False" title="(in Python v3.10)"><code class="xref any docutils literal notranslate"><span class="pre">False</span></code></a>, the default) w.r.t. outputs.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>posterior mean (shape [N, Q]) and (co)variance (shape as above) at test points</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="gpflux.layers.GPLayer.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">gpflow.base.TensorType</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/Tensor" title="(in TensorFlow v2.4)"><span class="pre">tf.Tensor</span></a></span></span><a class="headerlink" href="#gpflux.layers.GPLayer.call" title="Permalink to this definition">#</a></dt>
<dd><p>The default behaviour upon calling this layer.</p>
<p>This method calls the <a class="reference external" href="https://www.tensorflow.org/probability/api_docs/python/tfp/layers/DistributionLambda" title="(in TensorFlow Probability v0.12)"><code class="xref any docutils literal notranslate"><span class="pre">tfp.layers.DistributionLambda</span></code></a> super-class
<a class="reference internal" href="#gpflux.layers.GPLayer.call" title="gpflux.layers.GPLayer.call"><code class="xref any py py-meth docutils literal notranslate"><span class="pre">call</span></code></a> method, which constructs a <a class="reference external" href="https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/Distribution" title="(in TensorFlow Probability v0.12)"><code class="xref any docutils literal notranslate"><span class="pre">tfp.distributions.Distribution</span></code></a>
for the predictive distributions at the input points
(see <a class="reference internal" href="#gpflux.layers.GPLayer._make_distribution_fn" title="gpflux.layers.GPLayer._make_distribution_fn"><code class="xref py py-meth docutils literal notranslate"><span class="pre">_make_distribution_fn()</span></code></a>).
You can pass this distribution to <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/convert_to_tensor" title="(in TensorFlow v2.4)"><code class="xref any docutils literal notranslate"><span class="pre">tf.convert_to_tensor</span></code></a>, which will return
samples from the distribution (see <a class="reference internal" href="#gpflux.layers.GPLayer._convert_to_tensor_fn" title="gpflux.layers.GPLayer._convert_to_tensor_fn"><code class="xref py py-meth docutils literal notranslate"><span class="pre">_convert_to_tensor_fn()</span></code></a>).</p>
<p>This method also adds a layer-specific loss function, given by the KL divergence between
this layer and the GP prior (scaled to per-datapoint).</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="gpflux.layers.GPLayer.prior_kl">
<span class="sig-name descname"><span class="pre">prior_kl</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/Tensor" title="(in TensorFlow v2.4)"><span class="pre">tf.Tensor</span></a></span></span><a class="headerlink" href="#gpflux.layers.GPLayer.prior_kl" title="Permalink to this definition">#</a></dt>
<dd><p>Returns the KL divergence <code class="docutils literal notranslate"><span class="pre">KL[q(u)∥p(u)]</span></code> from the prior <code class="docutils literal notranslate"><span class="pre">p(u)</span></code> to
the variational distribution <code class="docutils literal notranslate"><span class="pre">q(u)</span></code>.  If this layer uses the
<a class="reference internal" href="#gpflux.layers.GPLayer.whiten" title="gpflux.layers.GPLayer.whiten"><code class="xref py py-attr docutils literal notranslate"><span class="pre">whiten</span></code></a>ed representation, returns <code class="docutils literal notranslate"><span class="pre">KL[q(v)∥p(v)]</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="gpflux.layers.GPLayer._make_distribution_fn">
<span class="sig-name descname"><span class="pre">_make_distribution_fn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">previous_layer_outputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">gpflow.base.TensorType</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/Distribution" title="(in TensorFlow Probability v0.12)"><span class="pre">tfp.distributions.Distribution</span></a></span></span><a class="headerlink" href="#gpflux.layers.GPLayer._make_distribution_fn" title="Permalink to this definition">#</a></dt>
<dd><p>Construct the posterior distributions at the output points of the previous layer,
depending on <a class="reference internal" href="#gpflux.layers.GPLayer.full_cov" title="gpflux.layers.GPLayer.full_cov"><code class="xref py py-attr docutils literal notranslate"><span class="pre">full_cov</span></code></a> and <a class="reference internal" href="#gpflux.layers.GPLayer.full_output_cov" title="gpflux.layers.GPLayer.full_output_cov"><code class="xref py py-attr docutils literal notranslate"><span class="pre">full_output_cov</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>previous_layer_outputs</strong> – The output from the previous layer,
which should be coercible to a <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/Tensor" title="(in TensorFlow v2.4)"><code class="xref any docutils literal notranslate"><span class="pre">tf.Tensor</span></code></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="gpflux.layers.GPLayer._convert_to_tensor_fn">
<span class="sig-name descname"><span class="pre">_convert_to_tensor_fn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distribution</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/Distribution" title="(in TensorFlow Probability v0.12)"><span class="pre">tfp.distributions.Distribution</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/Tensor" title="(in TensorFlow v2.4)"><span class="pre">tf.Tensor</span></a></span></span><a class="headerlink" href="#gpflux.layers.GPLayer._convert_to_tensor_fn" title="Permalink to this definition">#</a></dt>
<dd><p>Convert the predictive distributions at the input points (see
<a class="reference internal" href="#gpflux.layers.GPLayer._make_distribution_fn" title="gpflux.layers.GPLayer._make_distribution_fn"><code class="xref py py-meth docutils literal notranslate"><span class="pre">_make_distribution_fn()</span></code></a>) to a tensor of <a class="reference internal" href="#gpflux.layers.GPLayer.num_samples" title="gpflux.layers.GPLayer.num_samples"><code class="xref py py-attr docutils literal notranslate"><span class="pre">num_samples</span></code></a>
samples from that distribution.
Whether the samples are correlated or marginal (uncorrelated) depends
on <a class="reference internal" href="#gpflux.layers.GPLayer.full_cov" title="gpflux.layers.GPLayer.full_cov"><code class="xref py py-attr docutils literal notranslate"><span class="pre">full_cov</span></code></a> and <a class="reference internal" href="#gpflux.layers.GPLayer.full_output_cov" title="gpflux.layers.GPLayer.full_output_cov"><code class="xref py py-attr docutils literal notranslate"><span class="pre">full_output_cov</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="gpflux.layers.GPLayer.sample">
<span class="sig-name descname"><span class="pre">sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="../sampling/sample/index.html#gpflux.sampling.sample.Sample" title="gpflux.sampling.sample.Sample"><span class="pre">gpflux.sampling.sample.Sample</span></a></span></span><a class="headerlink" href="#gpflux.layers.GPLayer.sample" title="Permalink to this definition">#</a></dt>
<dd><div class="admonition-todo admonition" id="id1">
<p class="admonition-title">Todo</p>
<p>TODO: Document this.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="gpflux.layers.LatentVariableLayer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">LatentVariableLayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prior</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/Distribution" title="(in TensorFlow Probability v0.12)"><span class="pre">tfp.distributions.Distribution</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">encoder</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer" title="(in TensorFlow v2.4)"><span class="pre">tf.keras.layers.Layer</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">compositor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer" title="(in TensorFlow v2.4)"><span class="pre">tf.keras.layers.Layer</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/gpflux/layers/latent_variable_layer.html#LatentVariableLayer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#gpflux.layers.LatentVariableLayer" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#gpflux.layers.LayerWithObservations" title="gpflux.layers.LayerWithObservations"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LayerWithObservations</span></code></a></p>
<p>A latent variable layer, with amortized mean-field variational inference.</p>
<p>The latent variable is distribution-agnostic, but assumes a variational posterior
that is fully factorised and is of the same distribution family as the prior.</p>
<p>This class is used by models as described in <span id="id2">[<a class="reference internal" href="../../../index.html#id7" title="Vincent Dutordoir, Hugh Salimbeni, James Hensman, and Marc Deisenroth. Gaussian process conditional density estimation. In Advances in Neural Information Processing Systems. 2018.">DSHD18</a>, <a class="reference internal" href="../../../index.html#id8" title="Hugh Salimbeni, Vincent Dutordoir, James Hensman, and Marc Deisenroth. Deep Gaussian processes with importance-weighted variational inference. In International Conference on Machine Learning. 2019.">SDHD19</a>]</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>prior</strong> – A distribution that represents the <a class="reference internal" href="#gpflux.layers.LatentVariableLayer.prior" title="gpflux.layers.LatentVariableLayer.prior"><code class="xref py py-attr docutils literal notranslate"><span class="pre">prior</span></code></a> over the latent variable.</p></li>
<li><p><strong>encoder</strong> – A layer which is passed the concatenated observation inputs
and targets, and returns the appropriate parameters for the approximate
posterior distribution; see <a class="reference internal" href="#gpflux.layers.LatentVariableLayer.encoder" title="gpflux.layers.LatentVariableLayer.encoder"><code class="xref py py-attr docutils literal notranslate"><span class="pre">encoder</span></code></a>.</p></li>
<li><p><strong>compositor</strong> – A layer that combines layer inputs and latent variable
samples into a single tensor; see <a class="reference internal" href="#gpflux.layers.LatentVariableLayer.compositor" title="gpflux.layers.LatentVariableLayer.compositor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">compositor</span></code></a>. If you do not specify a value for
this parameter, the default is <code class="docutils literal notranslate"><span class="pre">tf.keras.layers.Concatenate(axis=-1)</span></code>.</p></li>
<li><p><strong>name</strong> – The name of this layer (passed through to <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer" title="(in TensorFlow v2.4)"><code class="xref any docutils literal notranslate"><span class="pre">tf.keras.layers.Layer</span></code></a>).</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="gpflux.layers.LatentVariableLayer.prior">
<span class="sig-name descname"><span class="pre">prior</span></span><em class="property"><span class="w"> </span><span class="pre">:tensorflow_probability.distributions.Distribution</span></em><a class="headerlink" href="#gpflux.layers.LatentVariableLayer.prior" title="Permalink to this definition">#</a></dt>
<dd><p>The prior distribution for the latent variables.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="gpflux.layers.LatentVariableLayer.encoder">
<span class="sig-name descname"><span class="pre">encoder</span></span><em class="property"><span class="w"> </span><span class="pre">:tensorflow.keras.layers.Layer</span></em><a class="headerlink" href="#gpflux.layers.LatentVariableLayer.encoder" title="Permalink to this definition">#</a></dt>
<dd><p>An encoder that maps from a concatenation of inputs and targets to the
parameters of the approximate posterior distribution of the corresponding
latent variables.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="gpflux.layers.LatentVariableLayer.compositor">
<span class="sig-name descname"><span class="pre">compositor</span></span><em class="property"><span class="w"> </span><span class="pre">:tensorflow.keras.layers.Layer</span></em><a class="headerlink" href="#gpflux.layers.LatentVariableLayer.compositor" title="Permalink to this definition">#</a></dt>
<dd><p>A layer that takes as input the two-element <code class="docutils literal notranslate"><span class="pre">[layer_inputs,</span> <span class="pre">latent_variable_samples]</span></code> list
and combines the elements into a single output tensor.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="gpflux.layers.LatentVariableLayer.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">gpflow.base.TensorType</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">observations</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">gpflux.types.ObservationType</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><span class="pre">bool</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><span class="pre">int</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/Tensor" title="(in TensorFlow v2.4)"><span class="pre">tf.Tensor</span></a></span></span><a class="headerlink" href="#gpflux.layers.LatentVariableLayer.call" title="Permalink to this definition">#</a></dt>
<dd><p>Sample the latent variables and compose them with the layer input.</p>
<p>When training, draw a sample of the latent variable from the posterior,
whose distribution is parameterised by the encoder mapping from the data.
Also add a KL divergence [posterior∥prior] to the losses.</p>
<p>When not training, draw a sample of the latent variable from the prior.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>layer_inputs</strong> – The output of the previous layer.</p></li>
<li><p><strong>observations</strong> – The <code class="docutils literal notranslate"><span class="pre">[inputs,</span> <span class="pre">targets]</span></code>, with the shapes <code class="docutils literal notranslate"><span class="pre">[batch</span> <span class="pre">size,</span> <span class="pre">Din]</span></code>
and <code class="docutils literal notranslate"><span class="pre">[batch</span> <span class="pre">size,</span> <span class="pre">Dout]</span></code> respectively. This parameter should be passed only when in
training mode.</p></li>
<li><p><strong>training</strong> – The training mode indicator.</p></li>
<li><p><strong>seed</strong> – A random seed for the sampling operation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Samples of the latent variable composed with the layer inputs through the
<a class="reference internal" href="#gpflux.layers.LatentVariableLayer.compositor" title="gpflux.layers.LatentVariableLayer.compositor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">compositor</span></code></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="gpflux.layers.LatentVariableLayer._inference_posteriors">
<span class="sig-name descname"><span class="pre">_inference_posteriors</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">observations</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">gpflux.types.ObservationType</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><span class="pre">bool</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/Distribution" title="(in TensorFlow Probability v0.12)"><span class="pre">tfp.distributions.Distribution</span></a></span></span><a class="headerlink" href="#gpflux.layers.LatentVariableLayer._inference_posteriors" title="Permalink to this definition">#</a></dt>
<dd><p>Return the posterior distributions parametrised by the <a class="reference internal" href="#gpflux.layers.LatentVariableLayer.encoder" title="gpflux.layers.LatentVariableLayer.encoder"><code class="xref py py-attr docutils literal notranslate"><span class="pre">encoder</span></code></a>, which gets called
with the concatenation of the inputs and targets in the <em>observations</em> argument.</p>
<div class="admonition-todo admonition" id="id3">
<p class="admonition-title">Todo</p>
<p>We might want to change encoders to have a
<a class="reference external" href="https://www.tensorflow.org/probability/api_docs/python/tfp/layers/DistributionLambda" title="(in TensorFlow Probability v0.12)"><code class="xref any docutils literal notranslate"><span class="pre">tfp.layers.DistributionLambda</span></code></a> final layer that directly returns the
appropriately parameterised distributions object.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>observations</strong> – The <code class="docutils literal notranslate"><span class="pre">[inputs,</span> <span class="pre">targets]</span></code>, with the shapes <code class="docutils literal notranslate"><span class="pre">[batch</span> <span class="pre">size,</span> <span class="pre">Din]</span></code>
and <code class="docutils literal notranslate"><span class="pre">[batch</span> <span class="pre">size,</span> <span class="pre">Dout]</span></code> respectively.</p></li>
<li><p><strong>training</strong> – The training mode indicator (passed through to the <a class="reference internal" href="#gpflux.layers.LatentVariableLayer.encoder" title="gpflux.layers.LatentVariableLayer.encoder"><code class="xref py py-attr docutils literal notranslate"><span class="pre">encoder</span></code></a>’s call).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The posterior distributions object.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="gpflux.layers.LatentVariableLayer._inference_latent_samples_and_loss">
<span class="sig-name descname"><span class="pre">_inference_latent_samples_and_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">gpflow.base.TensorType</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">observations</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">gpflux.types.ObservationType</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><span class="pre">int</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/Tensor" title="(in TensorFlow v2.4)"><span class="pre">tf.Tensor</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/Tensor" title="(in TensorFlow v2.4)"><span class="pre">tf.Tensor</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#gpflux.layers.LatentVariableLayer._inference_latent_samples_and_loss" title="Permalink to this definition">#</a></dt>
<dd><p>Sample latent variables during the <em>training</em> forward pass, hence requiring
the observations. Also return the KL loss per datapoint.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>layer_inputs</strong> – The output of the previous layer _(unused)_.</p></li>
<li><p><strong>observations</strong> – The <code class="docutils literal notranslate"><span class="pre">[inputs,</span> <span class="pre">targets]</span></code>, with the shapes <code class="docutils literal notranslate"><span class="pre">[batch</span> <span class="pre">size,</span> <span class="pre">Din]</span></code>
and <code class="docutils literal notranslate"><span class="pre">[batch</span> <span class="pre">size,</span> <span class="pre">Dout]</span></code> respectively.</p></li>
<li><p><strong>seed</strong> – A random seed for the sampling operation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The samples and the loss-per-datapoint.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="gpflux.layers.LatentVariableLayer._prediction_latent_samples">
<span class="sig-name descname"><span class="pre">_prediction_latent_samples</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">gpflow.base.TensorType</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><span class="pre">int</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/Tensor" title="(in TensorFlow v2.4)"><span class="pre">tf.Tensor</span></a></span></span><a class="headerlink" href="#gpflux.layers.LatentVariableLayer._prediction_latent_samples" title="Permalink to this definition">#</a></dt>
<dd><p>Sample latent variables during the <em>prediction</em> forward pass, only
depending on the shape of this layer’s inputs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>layer_inputs</strong> – The output of the previous layer (for determining batch shape).</p></li>
<li><p><strong>seed</strong> – A random seed for the sampling operation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The samples.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="gpflux.layers.LatentVariableLayer._local_kls">
<span class="sig-name descname"><span class="pre">_local_kls</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">posteriors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/Distribution" title="(in TensorFlow Probability v0.12)"><span class="pre">tfp.distributions.Distribution</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/Tensor" title="(in TensorFlow v2.4)"><span class="pre">tf.Tensor</span></a></span></span><a class="headerlink" href="#gpflux.layers.LatentVariableLayer._local_kls" title="Permalink to this definition">#</a></dt>
<dd><p>Compute the KL divergences [posteriors∥prior].</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>posteriors</strong> – A distribution that represents the approximate posteriors.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The KL divergences from the prior for each of the posteriors.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="gpflux.layers.LayerWithObservations">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">LayerWithObservations</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">trainable</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dynamic</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/gpflux/layers/latent_variable_layer.html#LayerWithObservations"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#gpflux.layers.LayerWithObservations" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="trackable_layer/index.html#gpflux.layers.trackable_layer.TrackableLayer" title="gpflux.layers.trackable_layer.TrackableLayer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">gpflux.layers.trackable_layer.TrackableLayer</span></code></a></p>
<p>By inheriting from this class, Layers indicate that their <a class="reference internal" href="#gpflux.layers.LayerWithObservations.call" title="gpflux.layers.LayerWithObservations.call"><code class="xref py py-meth docutils literal notranslate"><span class="pre">call()</span></code></a>
method takes a second <em>observations</em> argument after the customary
<em>layer_inputs</em> argument.</p>
<p>This is used to distinguish which layers (unlike most standard Keras
layers) require the original inputs and/or targets during training.
For example, it is used by the amortized variational inference in the
<a class="reference internal" href="#gpflux.layers.LatentVariableLayer" title="gpflux.layers.LatentVariableLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">LatentVariableLayer</span></code></a>.</p>
<dl class="py method">
<dt class="sig sig-object py" id="gpflux.layers.LayerWithObservations.call">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">gpflow.base.TensorType</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">observations</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">gpflux.types.ObservationType</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><span class="pre">bool</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/Tensor" title="(in TensorFlow v2.4)"><span class="pre">tf.Tensor</span></a></span></span><a class="headerlink" href="#gpflux.layers.LayerWithObservations.call" title="Permalink to this definition">#</a></dt>
<dd><p>The <a class="reference internal" href="#gpflux.layers.LayerWithObservations.call" title="gpflux.layers.LayerWithObservations.call"><code class="xref py py-meth docutils literal notranslate"><span class="pre">call()</span></code></a> method of <a class="reference internal" href="#gpflux.layers.LayerWithObservations" title="gpflux.layers.LayerWithObservations"><code class="xref any py py-class docutils literal notranslate"><span class="pre">LayerWithObservations</span></code></a> subclasses should
accept a second argument, <em>observations</em>. In training mode, this will
be the <code class="docutils literal notranslate"><span class="pre">[inputs,</span> <span class="pre">targets]</span></code> of the training points; otherwise, it is <a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.10)"><code class="xref any docutils literal notranslate"><span class="pre">None</span></code></a>.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="gpflux.layers.LikelihoodLayer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">LikelihoodLayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">likelihood</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">gpflow.likelihoods.Likelihood</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/gpflux/layers/likelihood_layer.html#LikelihoodLayer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#gpflux.layers.LikelihoodLayer" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="trackable_layer/index.html#gpflux.layers.trackable_layer.TrackableLayer" title="gpflux.layers.trackable_layer.TrackableLayer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">gpflux.layers.trackable_layer.TrackableLayer</span></code></a></p>
<p>A Keras layer that wraps a GPflow <code class="xref py py-class docutils literal notranslate"><span class="pre">Likelihood</span></code>. This layer expects a
<a class="reference external" href="https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/MultivariateNormalDiag" title="(in TensorFlow Probability v0.12)"><code class="xref any docutils literal notranslate"><span class="pre">tfp.distributions.MultivariateNormalDiag</span></code></a> as its input, describing <code class="docutils literal notranslate"><span class="pre">q(f)</span></code>.
When training, calling this class computes the negative variational expectation
<span class="math notranslate nohighlight">\(-\mathbb{E}_{q(f)}[\log p(y|f)]\)</span> and adds it as a layer loss.
When not training, it computes the mean and variance of <code class="docutils literal notranslate"><span class="pre">y</span></code> under <code class="docutils literal notranslate"><span class="pre">q(f)</span></code>
using <code class="xref py py-meth docutils literal notranslate"><span class="pre">predict_mean_and_var()</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Use <strong>either</strong> this <a class="reference internal" href="#gpflux.layers.LikelihoodLayer" title="gpflux.layers.LikelihoodLayer"><code class="xref any py py-class docutils literal notranslate"><span class="pre">LikelihoodLayer</span></code></a> (together with
<a class="reference internal" href="../models/index.html#gpflux.models.DeepGP" title="gpflux.models.DeepGP"><code class="xref any py py-class docutils literal notranslate"><span class="pre">gpflux.models.DeepGP</span></code></a>) <strong>or</strong> <a class="reference internal" href="../losses/index.html#gpflux.losses.LikelihoodLoss" title="gpflux.losses.LikelihoodLoss"><code class="xref any py py-class docutils literal notranslate"><span class="pre">LikelihoodLoss</span></code></a> (e.g. together with a
<a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/Sequential" title="(in TensorFlow v2.4)"><code class="xref any docutils literal notranslate"><span class="pre">tf.keras.Sequential</span></code></a> model). Do <strong>not</strong> use both at once because
this would add the loss twice.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="gpflux.layers.LikelihoodLayer.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/MultivariateNormalDiag" title="(in TensorFlow Probability v0.12)"><span class="pre">tfp.distributions.MultivariateNormalDiag</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">targets</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">gpflow.base.TensorType</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="likelihood_layer/index.html#gpflux.layers.likelihood_layer.LikelihoodOutputs" title="gpflux.layers.likelihood_layer.LikelihoodOutputs"><span class="pre">LikelihoodOutputs</span></a></span></span><a class="headerlink" href="#gpflux.layers.LikelihoodLayer.call" title="Permalink to this definition">#</a></dt>
<dd><p>When training (<code class="docutils literal notranslate"><span class="pre">training=True</span></code>), this method computes variational expectations
(data-fit loss) and adds this information as a layer loss.
When testing (the default), it computes the posterior mean and variance of <code class="docutils literal notranslate"><span class="pre">y</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inputs</strong> – The output distribution of the previous layer. This is currently
expected to be a <a class="reference external" href="https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/MultivariateNormalDiag" title="(in TensorFlow Probability v0.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultivariateNormalDiag</span></code></a>;
that is, the preceding <a class="reference internal" href="#gpflux.layers.GPLayer" title="gpflux.layers.GPLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPLayer</span></code></a> should have
<code class="docutils literal notranslate"><span class="pre">full_cov=full_output_cov=False</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a <a class="reference internal" href="likelihood_layer/index.html#gpflux.layers.likelihood_layer.LikelihoodOutputs" title="gpflux.layers.likelihood_layer.LikelihoodOutputs"><code class="xref any py py-class docutils literal notranslate"><span class="pre">LikelihoodOutputs</span></code></a> tuple with the mean and variance of <code class="docutils literal notranslate"><span class="pre">f</span></code> and,
if not training, the mean and variance of <code class="docutils literal notranslate"><span class="pre">y</span></code>.</p>
</dd>
</dl>
<div class="admonition-todo admonition" id="id4">
<p class="admonition-title">Todo</p>
<p>Turn this layer into a
<a class="reference external" href="https://www.tensorflow.org/probability/api_docs/python/tfp/layers/DistributionLambda" title="(in TensorFlow Probability v0.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistributionLambda</span></code></a> as well and return the
correct <a class="reference external" href="https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/Distribution" title="(in TensorFlow Probability v0.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Distribution</span></code></a> instead of a tuple
containing mean and variance only.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="gpflux.layers.TrackableLayer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">TrackableLayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">trainable</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dynamic</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/gpflux/layers/trackable_layer.html#TrackableLayer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#gpflux.layers.TrackableLayer" title="Permalink to this definition">#</a></dt>
<dd><p>Bases: <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer" title="(in TensorFlow v2.4)"><code class="docutils literal notranslate"><span class="pre">tf.keras.layers.Layer</span></code></a></p>
<p>With the release of TensorFlow 2.5, our TrackableLayer workaround is no
longer needed.  See <a class="reference external" href="https://github.com/Prowler-io/gpflux/issues/189">https://github.com/Prowler-io/gpflux/issues/189</a>.
Will be removed in GPflux version 1.0.0</p>
</dd></dl>

</section>
</section>


              </div>
              
              
          </main>
          

      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>
<footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright Copyright 2021 The GPflux Contributors

Licensed under the Apache License, Version 2.0
.<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.5.0.<br>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>