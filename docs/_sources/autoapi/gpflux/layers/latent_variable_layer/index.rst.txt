:py:mod:`gpflux.layers.latent_variable_layer`
=============================================

.. py:module:: gpflux.layers.latent_variable_layer

.. autoapi-nested-parse::

   This module implements a latent variable layer for deep GPs. 



Module Contents
---------------

.. py:class:: LayerWithObservations(trainable=True, name=None, dtype=None, dynamic=False, **kwargs)

   Bases: :py:obj:`gpflux.layers.trackable_layer.TrackableLayer`

   By inheriting from this class, Layers indicate that their :meth:`call`
   method takes a second *observations* argument after the customary
   *layer_inputs* argument.

   This is used to distinguish which layers (unlike most standard Keras
   layers) require the original inputs and/or targets during training.
   For example, it is used by the amortized variational inference in the
   :class:`LatentVariableLayer`.

   .. py:method:: call(self, layer_inputs: gpflow.base.TensorType, observations: Optional[gpflux.types.ObservationType] = None, training: Optional[bool] = None) -> tensorflow.Tensor
      :abstractmethod:

      The :meth:`call` method of `LayerWithObservations` subclasses should
      accept a second argument, *observations*. In training mode, this will
      be the ``[inputs, targets]`` of the training points; otherwise, it is `None`.



.. py:class:: LatentVariableLayer(prior: tensorflow_probability.distributions.Distribution, encoder: tensorflow.keras.layers.Layer, compositor: Optional[tensorflow.keras.layers.Layer] = None, name: Optional[str] = None)

   Bases: :py:obj:`LayerWithObservations`

   A latent variable layer, with amortized mean-field variational inference.

   The latent variable is distribution-agnostic, but assumes a variational posterior
   that is fully factorised and is of the same distribution family as the prior.

   This class is used by models as described in :cite:p:`dutordoir2018cde, salimbeni2019iwvi`.

   :param prior: A distribution that represents the :attr:`prior` over the latent variable.
   :param encoder: A layer which is passed the concatenated observation inputs
       and targets, and returns the appropriate parameters for the approximate
       posterior distribution; see :attr:`encoder`.
   :param compositor: A layer that combines layer inputs and latent variable
       samples into a single tensor; see :attr:`compositor`. If you do not specify a value for
       this parameter, the default is ``tf.keras.layers.Concatenate(axis=-1)``.
   :param name: The name of this layer (passed through to `tf.keras.layers.Layer`).

   .. py:attribute:: prior
      :annotation: :tensorflow_probability.distributions.Distribution

      The prior distribution for the latent variables. 


   .. py:attribute:: encoder
      :annotation: :tensorflow.keras.layers.Layer

      An encoder that maps from a concatenation of inputs and targets to the
      parameters of the approximate posterior distribution of the corresponding
      latent variables.


   .. py:attribute:: compositor
      :annotation: :tensorflow.keras.layers.Layer

      A layer that takes as input the two-element ``[layer_inputs, latent_variable_samples]`` list
      and combines the elements into a single output tensor.


   .. py:method:: call(self, layer_inputs: gpflow.base.TensorType, observations: Optional[gpflux.types.ObservationType] = None, training: Optional[bool] = None, seed: Optional[int] = None) -> tensorflow.Tensor

      Sample the latent variables and compose them with the layer input.

      When training, draw a sample of the latent variable from the posterior,
      whose distribution is parameterised by the encoder mapping from the data.
      Also add a KL divergence [posterior∥prior] to the losses.

      When not training, draw a sample of the latent variable from the prior.

      :param layer_inputs: The output of the previous layer.
      :param observations: The ``[inputs, targets]``, with the shapes ``[batch size, Din]``
          and ``[batch size, Dout]`` respectively. This parameter should be passed only when in
          training mode.
      :param training: The training mode indicator.
      :param seed: A random seed for the sampling operation.
      :returns: Samples of the latent variable composed with the layer inputs through the
          :attr:`compositor`


   .. py:method:: _inference_posteriors(self, observations: gpflux.types.ObservationType, training: Optional[bool] = None) -> tensorflow_probability.distributions.Distribution

      Return the posterior distributions parametrised by the :attr:`encoder`, which gets called
      with the concatenation of the inputs and targets in the *observations* argument.

      .. todo:: We might want to change encoders to have a
          `tfp.layers.DistributionLambda` final layer that directly returns the
          appropriately parameterised distributions object.

      :param observations: The ``[inputs, targets]``, with the shapes ``[batch size, Din]``
          and ``[batch size, Dout]`` respectively.
      :param training: The training mode indicator (passed through to the :attr:`encoder`'s call).
      :returns: The posterior distributions object.


   .. py:method:: _inference_latent_samples_and_loss(self, layer_inputs: gpflow.base.TensorType, observations: gpflux.types.ObservationType, seed: Optional[int] = None) -> Tuple[tensorflow.Tensor, tensorflow.Tensor]

      Sample latent variables during the *training* forward pass, hence requiring
      the observations. Also return the KL loss per datapoint.

      :param layer_inputs: The output of the previous layer _(unused)_.
      :param observations: The ``[inputs, targets]``, with the shapes ``[batch size, Din]``
          and ``[batch size, Dout]`` respectively.
      :param seed: A random seed for the sampling operation.
      :returns: The samples and the loss-per-datapoint.


   .. py:method:: _prediction_latent_samples(self, layer_inputs: gpflow.base.TensorType, seed: Optional[int] = None) -> tensorflow.Tensor

      Sample latent variables during the *prediction* forward pass, only
      depending on the shape of this layer's inputs.

      :param layer_inputs: The output of the previous layer (for determining batch shape).
      :param seed: A random seed for the sampling operation.
      :returns: The samples.


   .. py:method:: _local_kls(self, posteriors: tensorflow_probability.distributions.Distribution) -> tensorflow.Tensor

      Compute the KL divergences [posteriors∥prior].

      :param posteriors: A distribution that represents the approximate posteriors.
      :returns: The KL divergences from the prior for each of the posteriors.



